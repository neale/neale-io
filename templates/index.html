<!DOCTYPE html>
{% extends "base.html" %}

{% block title %}
Profile
{% endblock %}


{% block content %}

<link rel="stylesheet" type="text/css" href="{{ url_for('static',filename='css/index.css') }}">
    <header class="page-header" role="banner">
      <h1 class="project-name">Neale Ratzlaff</h1>
      <h2 class="project-tagline">Implicit is Sometimes Better than Explicit</h2>
      
      
      <a href="#" class="image-avatar"> <img src="images/selfpic_circle2.png" width=alt=""></a>
    </header>

    <main id="content" class="main-content" role="main">
      <p>I‚Äôm Neale Ratzlaff, üå≤ üå≤ a Deep Learning and Computer Vision researcher, I specialize in quantification of uncertainty in deep learning systemes, mostly working in computer vision and deep reinforcement learning</p>

      <p>I‚Äôm a research scientist at <a href="https://www.hrl.com/laboratories/issl">HRL Laboratories</a> in Malibu California.
I‚Äôm broadly interested in implicit generative modeling, approximate inference, scaling capabilities, and uncertainty quantification. My AGI timeline has been steadily getting shorter in recent years, and I welcome any opportunity to contribute to technical AI alignment.</p>

      <p>Beyond cold hard ML, I also work on more creative applications of neural networks for art generation. While
      large-scale text to image models are impressive, I perfer to use them in limited contexts, and create my own
      algorithms instead. Some of my work here can be seen in the <a href="https://neale.io/gallery">Gallery</a>. 


<p>I completed my at Oregon State university advised by Dr. <a href="https://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>. During my PhD I interned at <a href="https://horizon.ai/">Horizon Robotics</a> and <a href="https://vimeo.com/170280447">Intel</a></p>

<div style="text-align:center">
    <img width="20%" src="images/hrl_logo.png" /> 
    <img src="images/logo_horizonrobotics.png" width="25%"/>
    <img src="images/intel_logo.png" /> 


<h1 id="News">News</h1>
<ul>
    <li>I accepted a research scientist position at HRL Laboratories in Malibu, CA.</li>

    <li>Our paper that proposes a new method for identifying covariate shift in image data was accepted to IEEE VIS 2021</li>

    <li>I successfully defended my dissertation on uncertainty quantification in deep learning with implicit distributions over neural networks</li>
</ul>

<h1 id="ActiveProjects">‚õë ‚õë  Active Projects ‚õè ‚õè </h1>

<h3 id="neuralcanvas">Neural Canvas üñå ‚ö°</h3>

<p>Code: <a href=https://github.com/neale/neural-canvas>Github Repo</a>

<div style="text-align:center"><img src="images/gallery_imgs/montage.png" /></div>

<p>Neural canvas support much of the image generation projects that I take on. beyond the code here, which I am actively
developing and adding to, there is just a lot of labor and experimental features. A couple years ago I found that
implicit neural representations were pretty unsupported online beyond some NERF repositories, so I'm trying to remedy
that. While neural-canvas could be used in an academic research context, its got a lot of features that are aimed at
creative use. 
</p>

<h3 id="Agent Artist">Agent Artist ü§ó üñå ü§ó </h3>

<p>Code: <a href=https://github.com/neale/agent-artist>Github Repo</a>

<div style="text-align:center"><img src="images/gallery_imgs/art_factory.png" /></div>

<p>In the same vein as AutoGPT, the goal of agent-artist is to enable LLMs to act as agents toward arbitrary goals. 
The difference is two-fold:
</p>
<p> * Support multimodal understanding -- MiniGPT4 / BLIP2 understand images for example </p>
<p> * Support free and local models, where AutoGPT is focused on GPT3.5 and GPT4 </p>

<p>
True multimodal understanding enables a multimodal memory, instead of relying on text sumarization for recall and
synthesis. 
This work is still very much WIP
</p>


<h1 id="papers">üìöüìö Selected Pubications üìöüìö </h1>

<h3 id="generative-particle-variational-inference-via-estimation-of-functional-gradients"><a href="https://arxiv.org/abs/2103.01291">Generative Particle Variational Inference via Estimation of Functional Gradients</a></h3>

<p><strong>Ratzlaff</strong><sup>‚ò®</sup>, Bai<sup>‚ò®</sup>, Fuxin, Xu. (<strong>ICML</strong>) 2021(‚ò®): Equal Contribution</p>

<p>
Code: <a href="https://github.com/HorizonRobotics/alf/">Github Repo</a>
</p>

<p>We introduce a new method for Bayesian deep learning: GPVI, that fuses the flexibility of particle-based variational methods, with the efficiency of generative samplers. We construct a neural sampler that is trained with the functional gradient of the KL-divergence between the empirical sampling distribution and the target distribution. We show that GPVI accurately models the posterior distribution when applied to density estimation and Bayesian neural networks. This work also features a new method for estimating the inverse of the input-output jacobian without invertibility restrictions.</p>

<div style="text-align:center"><img src="images/2class-sin.png" /></div>

<hr />

<h3 id="constrastive-id"><a href=https://arxiv.org/abs/2108.08000">Contrastive Identification of Covariate Shift in Image Data</a></h3>

<p>Matthew L. Olson, Thuy-Vy Nguyen, Gaurav Dixit, <strong>Neale Ratzlaff</strong>, Weng-Keen Wong, Minsuk Kahng (<strong>IEEE VIS</strong>) 2021
</p>

We design and evaluate a new visual interface that facilitates the comparison of the local distributions of training and test data. We conduct a quantitative user study on multi-attribute facial data to compare the learned latent representations of ImageNet CNNs vs. density ratio models, with two user analytic workflows.
<div style="text-align:center"><img src="/images/vis_pic.png" /></div>


<h3 id="avoiding-side-effects-in-complex-environments"><a href="https://128.84.21.199/abs/2006.06547">Avoiding Side-Effects in Complex Environments.</a></h3>

<p>Turner<sup>‚ò®</sup>, <strong>Ratzlaff</strong><sup>‚ò®</sup>, Tadepalli. (<strong>NeurIPS</strong>) 2020, <strong>Spotlight talk</strong></p>

<p>(‚ò®): Equal Contribution</p>

<p>Code: <a href="https://github.com/neale/avoiding-side-effects">Github Repo</a></p>

<p>We introduce reinforcement learning agents that can accomplish goals without incurring side effects. Standard RL agents collect reward at any cost, often unsustainably ruining the environment around them. The attainable utility penalty (AUP) penalizes agents for acting in a way that decreases in their ability to achieve unknown future goals. We extend AUP to the deep RL case, and show that our AUP agents can act in difficult environments with stochastic dynamics, without incurring side effects.</p>

<div style="text-align:center"><img src="images/aup_paper.png" /></div>

<hr />

<h3 id="implicit-generative-modeling-for-efficient-exploration"><a href="https://arxiv.org/abs/1911.08017">Implicit Generative Modeling for Efficient Exploration.</a></h3>

<p><strong>Ratzlaff</strong>, Bai, Fuxin, Xu. (<strong>ICML</strong>) 2020</p>

<p>We model uncertainty estimation as an intrinsic reward for efficient exploration. We introduce an implicit generative modeling approach to estimate a Bayesian uncertainty of the agent‚Äôs belief of theenvironment dynamics.  We approximate the posterior through multiple draws from our generative model. The variance of the dynamic models‚Äô output is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on amortized Stein Variational Gradient Descent, to ensure the parameter distribution is a nontrivial approximation to the true posterior.</p>

<div style="text-align:center"><img src="images/RLpaper_hypergan.png" /></div>

<hr />

<h3 id="hypergan-a-generative-model-for-diverse-performant-neural-networks"><a href="http://proceedings.mlr.press/v97/ratzlaff19a/ratzlaff19a.pdf">HyperGAN: A Generative Model for Diverse Performant Neural Networks.</a></h3>

<p><strong>Ratzlaff</strong>, Fuxin. (<strong>ICML</strong>) 2019</p>

<p>Code: <a href="https://github.com/neale/HyperGAN">HyperGAN Github repo</a></p>

<p>Talk: ICML Oral <a href="https://slideslive.com/38917398/general-ml">Slideshare</a></p>

<p>We learn an implicit ensemble using a neural generating network. Trained by maximum likelihood, the generator learns to sample from the posterior of model parameters which fit the data. 
The generated model parameters achieve high accuracy, yet are distinct with different predictive distributions. 
We enforce diversity by regularizing the intermediate representations to be well-distributed, while not harming the flexibility of the output distribution.</p>

<div style="text-align:center"><img src="images/hypergan.png" /></div>

<hr />

<h3 id="unifying-bilateral-filtering-and-adversarial-training-for-robust-neural-networks"><a href="https://arxiv.org/abs/1804.01635">Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks.</a></h3>

<p><strong>Ratzlaff</strong>, Fuxin. (<em>arxiv</em> preprint) 2018</p>

<p>Code: <a href="https://github.com/neale/adversarial-toolbox">Github repo</a></p>

<p>We use an adaptive bilateral filter to smooth the purturbations left by adversarial attacks. We view our method as a piecewise projection of the high frequency perturbations, back to the natural image manifold. Our method is simple, effective, and practical, unlike many other projection defenses</p>

<p><img src="images/BFNet.png" alt="BFNet" /></p>

<hr />

<h1 id="theses">Theses</h1>

<h3 id="Uncertianty-in-deep-learning-with-implicit-neural-networks">
    <a href="https://ir.library.oregonstate.edu/concern/graduate_thesis_or_dissertations/1j92gf94q">Uncertainty in Deep Learning with Implicit Neural Networks</a></h3>

<p>
üéì PhD dissertation, Computer Science, Oregon State University (2021)
</p>

<h3 id="methods-for-detection-and-recovery-of-out-of-distribution-examples-ms-degree-computer-science-oregon-state-university-2018"><a href="https://ir.library.oregonstate.edu/concern/graduate_thesis_or_dissertations/mw22vb88d">Methods for Detection and Recovery of Out-of-Distribution Examples.</a> </h3>

<p>
üéì M.S. Thesis Computer Science. Oregon State University (2018)
</p>
<div style="text-align:center"><img src="images/class.png" /> <img src="images/density.png" /> </div>

<hr />

<h1 id="code">Older Code</h1>

<p>I maintain and work on various other projects, all deep learning related. Pretty much all of them are in PyTorch.</p>

<ul>
  <li><a href="https://github.com/neale/adversarial-autoencoder">Adversarial Autoencoders</a></li>
  <li><a href="https://github.com/neale/Improved-WGAN">Improved Wasserstein GAN</a></li>
  <li><a href="https://github.com/neale/CPPN">Compositional Pattern Producing Networks</a></li>
  <li><a href="https://github.com/neale/PySMTDNN">Satisfiable Neural Networks (PySMT-DNN)</a></li>
</ul>
<hr />

  {% endblock %}
</html>
